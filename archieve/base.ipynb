{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.2.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.31-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.9.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.10 (from langchain)\n",
      "  Downloading langchain_core-0.2.10-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.82-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from langchain)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting pydantic<3,>=1 (from langchain)\n",
      "  Using cached pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.4.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.10->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/pruthvirajadhav/miniconda3/envs/mars/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (24.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.5-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.4 (from pydantic<3,>=1->langchain)\n",
      "  Downloading pydantic_core-2.18.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/pruthvirajadhav/miniconda3/envs/mars/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain)\n",
      "  Using cached certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading langchain-0.2.6-py3-none-any.whl (975 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.5-cp312-cp312-macosx_11_0_arm64.whl (392 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m392.4/392.4 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.2.10-py3-none-any.whl (332 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.8/332.8 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.82-py3-none-any.whl (127 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Downloading pydantic-2.7.4-py3-none-any.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.0/409.0 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.18.4-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading SQLAlchemy-2.0.31-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.4.2-py3-none-any.whl (28 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Downloading frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Downloading orjson-3.10.5-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (258 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.9/258.9 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Downloading yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.4/79.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: urllib3, tenacity, SQLAlchemy, PyYAML, pydantic-core, orjson, numpy, multidict, jsonpointer, idna, frozenlist, charset-normalizer, certifi, attrs, annotated-types, yarl, requests, pydantic, jsonpatch, aiosignal, langsmith, aiohttp, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.31 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 attrs-23.2.0 certifi-2024.6.2 charset-normalizer-3.3.2 frozenlist-1.4.1 idna-3.7 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.6 langchain-core-0.2.10 langchain-text-splitters-0.2.2 langsmith-0.1.82 multidict-6.0.5 numpy-1.26.4 orjson-3.10.5 pydantic-2.7.4 pydantic-core-2.18.4 requests-2.32.3 tenacity-8.4.2 urllib3-2.2.2 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/pruthvirajadhav/miniconda3/envs/mars/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (0.2.6)\n",
      "Collecting psycopg2-binary (from -r requirements.txt (line 2))\n",
      "  Using cached psycopg2_binary-2.9.9-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.4 kB)\n",
      "Collecting python-dotenv (from -r requirements.txt (line 3))\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting openai (from -r requirements.txt (line 4))\n",
      "  Using cached openai-1.35.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting google-generativeai (from -r requirements.txt (line 5))\n",
      "  Using cached google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting supabase (from -r requirements.txt (line 6))\n",
      "  Using cached supabase-2.5.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting tiktoken (from -r requirements.txt (line 7))\n",
      "  Using cached tiktoken-0.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting unstructured (from -r requirements.txt (line 8))\n",
      "  Using cached unstructured-0.14.9-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting ssl (from -r requirements.txt (line 9))\n",
      "  Using cached ssl-1.16.tar.gz (33 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[7 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/dy/lmyr5nbs17998vfdw942gttr0000gn/T/pip-install-whk5nzim/ssl_53ccb38cd21d4a9b9864cb801360a113/setup.py\", line 33\n",
      "  \u001b[31m   \u001b[0m     print 'looking for', f\n",
      "  \u001b[31m   \u001b[0m     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m SyntaxError: Missing parentheses in call to 'print'. Did you mean print(...)?\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /Users/pruthvirajadhav/miniconda3/envs/mars/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pruthvirajadhav/miniconda3/envs/mars/lib/python3.12/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/pruthvirajadhav/miniconda3/envs/mars/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.5.15-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Users/pruthvirajadhav/miniconda3/envs/mars/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/pruthvirajadhav/miniconda3/envs/mars/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pruthvirajadhav/miniconda3/envs/mars/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pruthvirajadhav/miniconda3/envs/mars/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pruthvirajadhav/miniconda3/envs/mars/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pruthvirajadhav/miniconda3/envs/mars/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\n",
      "Downloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "Using cached regex-2024.5.15-cp312-cp312-macosx_11_0_arm64.whl (278 kB)\n",
      "Using cached safetensors-0.4.3-cp312-cp312-macosx_11_0_arm64.whl (411 kB)\n",
      "Using cached tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.15.4 fsspec-2024.6.1 huggingface-hub-0.23.4 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.42.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "# from chromadb.config import Settings\n",
    "# import chromadb\n",
    "import argparse\n",
    "import time\n",
    "from transformers import pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to show chain of thought\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain import PromptTemplate, FewShotPromptTemplate, OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.schema import Document\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "# Prefix of the prompt\n",
    "prefix = \"You are a helpful chatbot and answer questions based on provided context only. If the answer to the question is not there in the context, you can politely say that you do not have the answer\"\n",
    "\n",
    "# Examples of chain of thought to be included in the prompt\n",
    "EXAMPLES=[\"\"\"Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be based on {context}\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\"\"\"]\n",
    "\n",
    "# Template to be used\n",
    "example_template =\"\"\"\n",
    "Context: {context}\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables = [\"context\",\"query\",\"answer\"],\n",
    "    template = example_template\n",
    ")\n",
    "\n",
    "suffix = \"\"\"\n",
    "Context: {context}\n",
    "User: {query}\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "CHAT_PROMPT = PromptTemplate.from_examples(\n",
    "    examples = EXAMPLES, suffix=suffix, input_variables=[\"context\",\"query\"],prefix=prefix\n",
    ")\n",
    "\n",
    "# query = \"I want to buy stocks of Google. Can I buy through your bank\"\n",
    "# context = \"Bank customers will not be able to trade in shares and mutual funds through their bank account. \" \\\n",
    "#           \"They will need to open a trading account for trading in the market\"\n",
    "\n",
    "#print(CHAT_PROMPT.format(query=query,context=context))\n",
    "\n",
    "#query = \"I want to buy stocks of Google. Can I buy through your bank\"\n",
    "#query = \"Is the bank open on 25th December\"\n",
    "# context = \"Langchain in a python based llm framework. It was created in 2023 by Harrison chase\"\n",
    "#print(TEXTWORLD_PROMPT.format(query=query,context=context))\n",
    "\n",
    " \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "chain = load_qa_chain(llm,chain_type=\"stuff\",prompt=CHAT_PROMPT,verbose=False)\n",
    "docs = [Document(page_content=\"Bank customers will not be able to trade in shares and mutual funds through their bank \"\n",
    "                              \"account. They will need to open and trading account for trading in the market\",\n",
    "                 metadata={}),\n",
    "        Document(page_content=\"Bank customers can open trading account by logging into the banks portal\",\n",
    "                 metadata={})\n",
    "        ]\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "   \n",
    "embeddings = HuggingFaceEmbeddings(model_name= \"all-MiniLM-L6-v2\")\n",
    "    \n",
    "persist_directory =\"db\"\n",
    "CHROMA_SETTINGS = Settings(\n",
    "        persist_directory=persist_directory,\n",
    "        anonymized_telemetry=False\n",
    ")\n",
    "chroma_client = chromadb.PersistentClient(settings=CHROMA_SETTINGS , path=persist_directory)\n",
    "db = Chroma(persist_directory=persist_directory, embedding_function=embeddings, \n",
    "            client_settings=CHROMA_SETTINGS, client=chroma_client)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": target_source_chunks})\n",
    "    # activate/deactivate the streaming StdOut callback for LLMs\n",
    "callbacks = [] if args.mute_stream else [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(   \n",
    "llm=llm,   \n",
    "chain_type=\"stuff\",   \n",
    "retriever=retriever ,\n",
    "chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "callbacks=callbacks,\n",
    "    return_source_documents= not args.hide_source\n",
    "\n",
    "while True:\n",
    "    query = input(\"What is your question:\\n\")\n",
    "    response = chain.run(input_documents=docs,query=query)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.text_splitter\n",
    "langchain.embeddings\n",
    "langchain.llms\n",
    "langchain.vectorstores\n",
    "langchain.chains\n",
    "langchain.memory\n",
    "langchain_core.messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
